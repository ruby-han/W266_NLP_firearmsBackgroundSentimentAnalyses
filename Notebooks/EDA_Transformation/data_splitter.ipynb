{"cells":[{"cell_type":"markdown","metadata":{"id":"nR8dyx0AHsVb"},"source":["### Import Packages"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"W5paQvvxX1BS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1658361235067,"user_tz":420,"elapsed":51777,"user":{"displayName":"Gerrit Lensink","userId":"03001139976263396963"}},"outputId":"b8697ffc-eacb-48d7-ba35-8bdacc325baa"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[K     |████████████████████████████████| 24.2 MB 19.4 MB/s \n","\u001b[K     |████████████████████████████████| 4.9 MB 5.0 MB/s \n","\u001b[K     |████████████████████████████████| 4.4 MB 5.0 MB/s \n","\u001b[K     |████████████████████████████████| 6.6 MB 34.2 MB/s \n","\u001b[K     |████████████████████████████████| 596 kB 36.6 MB/s \n","\u001b[K     |████████████████████████████████| 101 kB 9.1 MB/s \n","\u001b[K     |████████████████████████████████| 1.1 MB 4.9 MB/s \n","\u001b[?25h"]}],"source":["#@title Imports\n","\n","!pip install pydot --quiet\n","!pip install gensim==3.8.3 --quiet\n","!pip install tensorflow-datasets --quiet\n","!pip install -U tensorflow-text==2.8.2 --quiet\n","!pip install transformers --quiet\n","!pip install pydot --quiet\n","!pip install tensorflow_addons --quiet"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"yVCGGpRCDmWU","executionInfo":{"status":"ok","timestamp":1658361247594,"user_tz":420,"elapsed":12533,"user":{"displayName":"Gerrit Lensink","userId":"03001139976263396963"}}},"outputs":[],"source":["# Import packages\n","import pandas as pd\n","import numpy as np\n","from tensorflow import keras\n","\n","from tensorflow.keras.layers import Embedding, Input, Dense, Lambda\n","from tensorflow.keras.models import Model\n","import tensorflow.keras.backend as K\n","import tensorflow_datasets as tfds\n","import tensorflow_text as tf_text\n","import tensorflow_addons as tfa\n","\n","from google.colab import drive\n","\n","from collections import Counter\n","import matplotlib.pyplot as plt\n","from nltk.util import ngrams\n","\n","from transformers import BertTokenizer, TFBertModel\n","from tqdm.notebook import tqdm\n","from keras.preprocessing.sequence import pad_sequences\n","\n","from sklearn.model_selection import train_test_split\n","import tensorflow as tf\n","\n","\n","import time\n","from transformers import create_optimizer\n","\n","import sklearn as sk\n","import os\n","import nltk\n","from nltk.corpus import reuters\n","from nltk.data import find\n","\n","import re\n","\n","import gensim\n","from sklearn.metrics.pairwise import cosine_similarity\n","from transformers import BertModel\n"]},{"cell_type":"code","source":["pd.set_option(\"display.max_rows\", None, # display all rows\n","              \"display.max_columns\", None, # display all columns\n","              \"display.max_colwidth\", None, # expand column width\n","              \"display.html.use_mathjax\", False) # disable Latex style mathjax rendering"],"metadata":{"id":"Mv7MzE4WmHMf","executionInfo":{"status":"ok","timestamp":1658361247594,"user_tz":420,"elapsed":6,"user":{"displayName":"Gerrit Lensink","userId":"03001139976263396963"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","execution_count":4,"metadata":{"id":"NFPBnrozD6RK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1658361267888,"user_tz":420,"elapsed":20298,"user":{"displayName":"Gerrit Lensink","userId":"03001139976263396963"}},"outputId":"81329494-6adc-4321-aabf-0d3462759d85"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /drive\n","/drive/MyDrive/W266 Project/Colab Notebooks/Exploration\n","/drive/MyDrive/W266 Project/Colab Notebooks/Exploration\n"]}],"source":["drive.mount('/drive') \n","%cd /drive/MyDrive/W266 Project/Colab Notebooks/Exploration\n","!pwd"]},{"cell_type":"code","source":["t = pd.read_csv('../../data/transformed/final/test.csv').sample(frac = 1, \n","                                                                  random_state = 2) # shuffle rows"],"metadata":{"id":"yBSdXdQ-CCYo","executionInfo":{"status":"ok","timestamp":1658361272677,"user_tz":420,"elapsed":2162,"user":{"displayName":"Gerrit Lensink","userId":"03001139976263396963"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["len(t)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2sSwIxWPCO4o","executionInfo":{"status":"ok","timestamp":1658361281085,"user_tz":420,"elapsed":130,"user":{"displayName":"Gerrit Lensink","userId":"03001139976263396963"}},"outputId":"d9df3164-c9f5-47d6-b8fe-14555a94598e"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["63978"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","execution_count":7,"metadata":{"id":"eWpis5aUnysI","executionInfo":{"status":"ok","timestamp":1658361394458,"user_tz":420,"elapsed":2554,"user":{"displayName":"Gerrit Lensink","userId":"03001139976263396963"}}},"outputs":[],"source":["train = pd.read_csv('../../data/transformed/final/train.csv').sample(frac = 1, \n","                                                                          random_state = 2) # shuffle rows\n","val_test = pd.read_csv('../../data/transformed/final/test.csv').sample(frac = 1, \n","                                                                  random_state = 2) # shuffle rows"]},{"cell_type":"code","source":["# up until this point - the biggest set of data we used was 100k (with an 80/20 train/val split). \n","# So we need to make sure we retain the 20k test observations we've been using as val\n","# Luckily, we've been using random seeds, so when we sample again - we're assured it's the same val\n","\n","min_val = val_test.sample(n = int(100000 * .2),\n","                          random_state = 2)"],"metadata":{"id":"FB40A_8pCs11","executionInfo":{"status":"ok","timestamp":1658361551599,"user_tz":420,"elapsed":229,"user":{"displayName":"Gerrit Lensink","userId":"03001139976263396963"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["# This means that the remaining ~43k observations can be split between val or test, without worrying about contamination\n","# We will select all of the remaining indices \n","val_index = min_val.index\n","total_index = val_test.index\n","remain_index = total_index.difference(val_index, sort = False)"],"metadata":{"id":"6utZ9qmlDVIs","executionInfo":{"status":"ok","timestamp":1658361612916,"user_tz":420,"elapsed":122,"user":{"displayName":"Gerrit Lensink","userId":"03001139976263396963"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["print(len(val_index), len(total_index), len(remain_index)) # quick check on totals"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lUUoPfQLDj00","executionInfo":{"status":"ok","timestamp":1658361634825,"user_tz":420,"elapsed":128,"user":{"displayName":"Gerrit Lensink","userId":"03001139976263396963"}},"outputId":"d2a48bc0-67d2-4339-a753-cee607066dc1"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["20000 63978 43978\n"]}]},{"cell_type":"code","source":["# now we can isolate the remaining data that can be split between val and test\n","remain_val_test = val_test.loc[remain_index]\n","\n","# Check to make sure none of these indices show up in val\n","rvt_index = remain_val_test.index\n","\n","a = rvt_index.intersection(val_index, sort = False)\n","b = val_index.intersection(rvt_index, sort = False)\n","print(f'intersection between val and our isolated set is {len(a)+len(b)} indices long')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9O7PxHstDwmS","executionInfo":{"status":"ok","timestamp":1658361775964,"user_tz":420,"elapsed":132,"user":{"displayName":"Gerrit Lensink","userId":"03001139976263396963"}},"outputId":"5de88886-8c3f-4383-e0a5-79aee2b62435"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["intersection between val and our isolated set is 0 indices long\n"]}]},{"cell_type":"code","source":["# Now, we need to split the remainder of the saved off data to an 80/20 train and 80/20 val split\n","\n","# print(f'We have {len(total_index)} test/val observations, and {len(val_index)} of them are already in train. \\n\\\n","#       That means for an 80/20 split with {int(len(total_index)*.8)} total obs, we need {int(len(total_index)*.8) - len(val_index)}\\n\\\n","#       more observations in train')\n","\n","print(f'We have {len(total_index)} test/val observations, and {len(val_index)} of them are already in val')\n","print(f'With a train size of {len(train)}, we need {int((len(train)/.8) - len(train))} in both val and test to achieve 80/20 splits')\n","print(f'With only {len(total_index)} total observations in test/val, we are {int((len(train)/.8) - len(train) - len(total_index)/2)} observations short in both val and test to achieve 80/20 splits')\n","print(f'We will do the best we can here, and hold {int(len(total_index)/2)} for train and val each.')\n","print(f'That means that since val already has {len(val_index)} obs, we need to add {int(len(total_index)/2 - len(val_index))} more')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Us9rfXbRETQ0","executionInfo":{"status":"ok","timestamp":1658362207181,"user_tz":420,"elapsed":5,"user":{"displayName":"Gerrit Lensink","userId":"03001139976263396963"}},"outputId":"4de92d64-af59-4720-ae64-97dd5489f42e"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["We have 63978 test/val observations, and 20000 of them are already in val\n","With a train size of 159571, we need 39892 in both val and test to achieve 80/20 splits\n","With only 63978 total observations in test/val, we are 7903 observations short in both val and test to achieve 80/20 splits\n","We will do the best we can here, and hold 31989 for train and val each.\n","That means that since val already has 20000 obs, we need to add 11989 more\n"]}]},{"cell_type":"code","source":["val_plus = remain_val_test.sample(n = 11989,\n","                              random_state = 2)\n","val_plus_index = val_plus.index\n","test_index = rvt_index.difference(val_plus_index, sort = False)\n","\n","# Test to to make sure no intersection\n","test_index.intersection(val_plus_index, sort = False)\n","val_plus_index.intersection(test_index, sort = False)\n","\n","test_index.intersection(val_index, sort = False)\n","val_index.intersection(test_index, sort = False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zh2S9XwVF4v8","executionInfo":{"status":"ok","timestamp":1658362335867,"user_tz":420,"elapsed":125,"user":{"displayName":"Gerrit Lensink","userId":"03001139976263396963"}},"outputId":"bb2c9167-9896-44c5-c13b-8081c70d0ea6"},"execution_count":28,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Int64Index([], dtype='int64')"]},"metadata":{},"execution_count":28}]},{"cell_type":"code","source":["# Isolate test\n","test = remain_val_test.loc[test_index]\n","\n","# Finally add min_train and train_plus\n","val = pd.concat([min_val, val_plus])\n","\n","# Now, we can write each of these back to CSV for final modeling\n","train.to_csv('../../data/transformed/final/full_data/train_final.csv', index=False)\n","test.to_csv('../../data/transformed/final/full_data/test_final.csv', index=False)\n","val.to_csv('../../data/transformed/final/full_data/val_final.csv', index=False)"],"metadata":{"id":"f_ofwR3TGaaZ","executionInfo":{"status":"ok","timestamp":1658362478924,"user_tz":420,"elapsed":4677,"user":{"displayName":"Gerrit Lensink","userId":"03001139976263396963"}}},"execution_count":34,"outputs":[]}],"metadata":{"colab":{"collapsed_sections":["UBXi2fAJe-yA"],"name":"data_splitter","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}